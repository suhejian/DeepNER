{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 420M/420M [00:10<00:00, 43.3MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer,BertForTokenClassification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.add_tokens(['B_geo','I_geo','B_per','I_per','B_org','I_org'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/ner.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iranian officials say they expect to get acces...</td>\n",
       "      <td>B-gpe O O O O O O O O O O O O O O B-tim O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Helicopter gunships Saturday pounded militant ...</td>\n",
       "      <td>O O B-tim O O O O O B-geo O O O O O B-org O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They left after a tense hour-long standoff wit...</td>\n",
       "      <td>O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.N. relief coordinator Jan Egeland said Sunda...</td>\n",
       "      <td>B-geo O O B-per I-per O B-tim O B-geo O B-gpe ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Thousands of demonstrators have marched throug...   \n",
       "1  Iranian officials say they expect to get acces...   \n",
       "2  Helicopter gunships Saturday pounded militant ...   \n",
       "3  They left after a tense hour-long standoff wit...   \n",
       "4  U.N. relief coordinator Jan Egeland said Sunda...   \n",
       "\n",
       "                                              labels  \n",
       "0  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n",
       "1  B-gpe O O O O O O O O O O O O O O B-tim O O O ...  \n",
       "2  O O B-tim O O O O O B-geo O O O O O B-org O O ...  \n",
       "3                              O O O O O O O O O O O  \n",
       "4  B-geo O O B-per I-per O B-tim O B-geo O B-gpe ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(data): \n",
    "    '''\n",
    "        Here we will remove all the tags except for 'Org','Geo', and 'Per'\n",
    "        type. These three are our targeted Entities.\n",
    "    '''\n",
    "#     data['labels'] = data['labels'].str.replace('-','_')\n",
    "    data['labels'] = data['labels'].str.replace('B_gpe','O')\n",
    "    data['labels'] = data['labels'].str.replace('I_gpe','O')\n",
    "    data['labels'] = data['labels'].str.replace('B_tim','O')\n",
    "    data['labels'] = data['labels'].str.replace('I_tim','O')\n",
    "    data['labels'] = data['labels'].str.replace('B_eve','O')\n",
    "    data['labels'] = data['labels'].str.replace('I_eve','O')\n",
    "    data['labels'] = data['labels'].str.replace('B_nat','O')\n",
    "    data['labels'] = data['labels'].str.replace('I_nat','O')\n",
    "    data['labels'] = data['labels'].str.replace('B_art','O')\n",
    "    data['labels'] = data['labels'].str.replace('I_art','O')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=preprocess_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7042\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is to remove the sentences that doesn't contain our targeted entities.\n",
    "'''\n",
    "sum=0\n",
    "for index, i in enumerate(data['labels']):\n",
    "    a=set(i.split(' '))\n",
    "    if(len(a)<=1):\n",
    "        data.drop(labels=index, axis=0,inplace=True)\n",
    "        sum+=1    \n",
    "print(sum)\n",
    "data = data.dropna()\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        770252.0\n",
       "B-geo     37644.0\n",
       "B-gpe     15870.0\n",
       "B-tim     20333.0\n",
       "B-org     20143.0\n",
       "I-geo      7414.0\n",
       "B-per     16990.0\n",
       "I-per     17251.0\n",
       "I-org     16784.0\n",
       "I-tim      6528.0\n",
       "B-art       402.0\n",
       "I-art       297.0\n",
       "B-nat       201.0\n",
       "I-gpe       198.0\n",
       "I-nat        51.0\n",
       "B-eve       308.0\n",
       "I-eve       253.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tags = data.labels.apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0)\n",
    "unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ids = {'B-geo': 1,\n",
    " 'B-org': 2,\n",
    " 'B-per': 3,\n",
    " 'I-geo': 4,\n",
    " 'I-org': 5,\n",
    " 'I-per': 6,\n",
    " 'O': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_label = {1:'B-geo',\n",
    " 2:'B-org',\n",
    " 3:'B-per',\n",
    " 4:'I-geo',\n",
    " 5:'I-org',\n",
    " 6:'I-per',\n",
    " 0:'O'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    \n",
    "#     print('##')\n",
    "#     print(sentence)\n",
    "#     print(text_labels)\n",
    "#     print('###')\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        \n",
    "        ## if sentence consist of more than 125 words, discard the later words.\n",
    "        if(len(tokenized_sentence)>=125):\n",
    "            return tokenized_sentence, labels\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ner_Data(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "#         print(\"dataloader initialized\")\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx)\n",
    "        sentence = self.data['text'][idx].strip().split()  \n",
    "        word_labels = self.data['labels'][idx].split(\" \") \n",
    "#         print(len(sentence))\n",
    "#         if(len(sentence)>64):\n",
    "#             sentence=sentence[:63]\n",
    "#             word_labels=word_labels[:63]\n",
    "# #         print(sentence)\n",
    "        t_sen, t_labl = tokenize_and_preserve_labels(sentence, word_labels, tokenizer)\n",
    "                \n",
    "        sen_code = tokenizer.encode_plus(t_sen,    \n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "            max_length = 128,  # maximum length of a sentence\n",
    "            pad_to_max_length=True,  # Add [PAD]s\n",
    "            return_attention_mask = True,  # Generate the attention mask\n",
    "#             return_tensors = 'pt'\n",
    "            )\n",
    "             \n",
    "            \n",
    "        labels = [-100]*128\n",
    "        for i, tok in enumerate(t_labl):\n",
    "#             tok = tokenizer.convert_ids_to_tokens(i)\n",
    "#             print(tok)\n",
    "#             print(tok)\n",
    "#             print(label_to_ids.get(tok))\n",
    "            if label_to_ids.get(tok) != None:\n",
    "                labels[i+1]=label_to_ids.get(tok)\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in sen_code.items()}\n",
    "        item['labels'] = torch.as_tensor(labels)\n",
    "\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Ner_Data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n",
      "tensor([-100,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0, -100,    0,    0,    0,    3,    0,    3,    3,    6,    6,    0,\n",
      "           0,    0,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[10]['input_ids']))\n",
    "print(len(train_data[10]['labels']))\n",
    "print(train_data[10]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  2739  4311  2405  4465  1999  1996  2142  2163  2360  1996 15727\n",
      "  2015  1010  2112  1997  2019  4866  2981  3189  2000  2022  2207  2006\n",
      "  6928  1010  6232  4697  1037  2280  2327 14895  2000  3187  1011  2236\n",
      " 12849  8873  4698  2078  1010  3841  2239  7367  6212  1010  1998  1996\n",
      "  1057  1012  1050  1012  2436  2002  3753  2008  2743  1996  3514  1011\n",
      "  2005  1011  2833  2565  1012   102     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "['[CLS]', 'news', 'reports', 'published', 'sunday', 'in', 'the', 'united', 'states', 'say', 'the', 'audit', '##s', ',', 'part', 'of', 'an', 'extensive', 'independent', 'report', 'to', 'be', 'released', 'on', 'monday', ',', 'critic', '##ize', 'a', 'former', 'top', 'aide', 'to', 'secretary', '-', 'general', 'ko', '##fi', 'anna', '##n', ',', 'ben', '##on', 'se', '##van', ',', 'and', 'the', 'u', '.', 'n', '.', 'office', 'he', 'headed', 'that', 'ran', 'the', 'oil', '-', 'for', '-', 'food', 'program', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "#####\n",
      "None\n",
      "O\n",
      "O\n",
      "O\n",
      "None\n",
      "O\n",
      "O\n",
      "B-geo\n",
      "I-geo\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "None\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "B-per\n",
      "B-per\n",
      "I-per\n",
      "I-per\n",
      "O\n",
      "B-per\n",
      "B-per\n",
      "I-per\n",
      "I-per\n",
      "O\n",
      "O\n",
      "O\n",
      "B-geo\n",
      "B-geo\n",
      "B-geo\n",
      "B-geo\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_data[148]\n",
    "print(train_data[148]['input_ids'].detach().numpy())\n",
    "print(tokenizer.convert_ids_to_tokens(train_data[148]['input_ids'].detach().numpy()))\n",
    "\n",
    "print('#####')\n",
    "for i in train_data[148]['labels'].detach().numpy():\n",
    "#     print(i)\n",
    "    print(ids_to_label.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = model =BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(label_to_ids))\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn2 = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_dataloader, model, optimizer):\n",
    "    size = len(train_dataloader.dataset)\n",
    "    train_loss =0\n",
    "    for i,sample in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "#         print(sample)\n",
    "        ids=sample['input_ids'].to(device)\n",
    "        mask=sample['attention_mask'].to(device)\n",
    "        labels = sample['labels'].to(device)\n",
    "        pred = model2(input_ids=ids, attention_mask=mask ,labels = labels )\n",
    "        loss = pred[0]\n",
    "        \n",
    "#         print(f\"loss: {loss.item()}\")\n",
    "        train_loss+=loss.item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if(i>0 and i % 500==0):\n",
    "            print(f\"loss: {train_loss/i:>4f}  [{i:>5d}/{size/32}]\")\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\suhejian01\\AppData\\Local\\Temp\\ipykernel_27936\\3017434973.py\", line 5, in <cell line: 3>\n",
      "    loss = train_loop(train_dataloader, model, optimizer)\n",
      "  File \"C:\\Users\\suhejian01\\AppData\\Local\\Temp\\ipykernel_27936\\2587769509.py\", line 10, in train_loop\n",
      "    pred = model2(input_ids=ids, attention_mask=mask ,labels = labels )\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 1742, in forward\n",
      "    outputs = self.bert(\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 996, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 585, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 472, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 402, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 290, in forward\n",
      "    key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 103, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 6.00 GiB total capacity; 3.41 GiB already allocated; 4.12 MiB free; 3.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1992, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 799, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 845, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, \"bg:ansiyellow\")\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\stack_data\\core.py\", line 424, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\pygments\\style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"d:\\Software\\Anaconda\\anaconda\\envs\\pytorch\\lib\\site-packages\\pygments\\style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "train_loss = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    loss = train_loop(train_dataloader, model, optimizer)\n",
    "    train_loss.append(loss)\n",
    "#     test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At least 13 people were killed in Indian Kashm...</td>\n",
       "      <td>O O O O O O O B-gpe O B-tim O O O O O O O B-gp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indian officials say several gunbattles erupte...</td>\n",
       "      <td>B-gpe O O O O O O O O O O O O O O B-geo I-geo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They say seven militants were killed in the cl...</td>\n",
       "      <td>O O O O O O O O O O O O O B-gpe O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In another incident , India 's military says f...</td>\n",
       "      <td>O O O O B-geo O O O O O O O O O O B-gpe O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The civilian deaths triggered protests by resi...</td>\n",
       "      <td>O O O O O O O O B-geo O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Indian Prime Minister Manmohan Singh is to hol...</td>\n",
       "      <td>B-gpe O O B-per I-per O O O O O B-gpe O O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>But several prominent Kashmiri separatists hav...</td>\n",
       "      <td>O O O B-gpe O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The American fast food chain KFC is famous aro...</td>\n",
       "      <td>O B-gpe O O O B-org O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hundreds of riot police surrounded about 30 de...</td>\n",
       "      <td>O O O O O O O O O O O O O O B-per I-per O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KFC stands for Kentucky Fried Chicken and foun...</td>\n",
       "      <td>B-org O O B-org I-org I-org O O O B-per I-per ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>In Kabul , Afghanistan , another KFC restauran...</td>\n",
       "      <td>O B-geo O B-geo O O B-org O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>But this KFC is short for Kabul Fried Chicken ...</td>\n",
       "      <td>O O B-org O O O B-org I-org I-org O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Rahimgul Sarawan reports from Kabul that while...</td>\n",
       "      <td>B-per I-per O O B-geo O O O B-org O O O B-per ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Brian Allen narrates .</td>\n",
       "      <td>B-per I-per O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Israel 's government is set to approve a plan ...</td>\n",
       "      <td>B-geo O O O O O O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Jerusalem Post reported Friday that Prime ...</td>\n",
       "      <td>O B-art I-art O B-tim O B-per I-per I-per I-pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The newspaper said the multi-year plan also in...</td>\n",
       "      <td>O O O O B-tim O O O O O O O O O O O O O O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The report quotes the head of the Defense Mini...</td>\n",
       "      <td>O O O O O O O B-org I-org I-org I-org I-org O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The report said Israel is in talks with severa...</td>\n",
       "      <td>O O O B-org O O O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Earlier , the same group burned a coffin and a...</td>\n",
       "      <td>O O O O O O O O O O O O B-gpe O B-per I-per O ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   At least 13 people were killed in Indian Kashm...   \n",
       "1   Indian officials say several gunbattles erupte...   \n",
       "2   They say seven militants were killed in the cl...   \n",
       "3   In another incident , India 's military says f...   \n",
       "4   The civilian deaths triggered protests by resi...   \n",
       "5   Indian Prime Minister Manmohan Singh is to hol...   \n",
       "6   But several prominent Kashmiri separatists hav...   \n",
       "7   The American fast food chain KFC is famous aro...   \n",
       "8   Hundreds of riot police surrounded about 30 de...   \n",
       "9   KFC stands for Kentucky Fried Chicken and foun...   \n",
       "10  In Kabul , Afghanistan , another KFC restauran...   \n",
       "11  But this KFC is short for Kabul Fried Chicken ...   \n",
       "12  Rahimgul Sarawan reports from Kabul that while...   \n",
       "13                             Brian Allen narrates .   \n",
       "14  Israel 's government is set to approve a plan ...   \n",
       "15  The Jerusalem Post reported Friday that Prime ...   \n",
       "16  The newspaper said the multi-year plan also in...   \n",
       "17  The report quotes the head of the Defense Mini...   \n",
       "18  The report said Israel is in talks with severa...   \n",
       "19  Earlier , the same group burned a coffin and a...   \n",
       "\n",
       "                                               labels  \n",
       "0   O O O O O O O B-gpe O B-tim O O O O O O O B-gp...  \n",
       "1   B-gpe O O O O O O O O O O O O O O B-geo I-geo ...  \n",
       "2               O O O O O O O O O O O O O B-gpe O O O  \n",
       "3   O O O O B-geo O O O O O O O O O O B-gpe O O O ...  \n",
       "4                     O O O O O O O O B-geo O O O O O  \n",
       "5   B-gpe O O B-per I-per O O O O O B-gpe O O O B-...  \n",
       "6                     O O O B-gpe O O O O O O O O O O  \n",
       "7             O B-gpe O O O B-org O O O O O O O O O O  \n",
       "8   O O O O O O O O O O O O O O B-per I-per O O O ...  \n",
       "9   B-org O O B-org I-org I-org O O O B-per I-per ...  \n",
       "10            O B-geo O B-geo O O B-org O O O O O O O  \n",
       "11  O O B-org O O O B-org I-org I-org O O O O O O ...  \n",
       "12  B-per I-per O O B-geo O O O B-org O O O B-per ...  \n",
       "13                                    B-per I-per O O  \n",
       "14        B-geo O O O O O O O O O O O O O O O O O O O  \n",
       "15  O B-art I-art O B-tim O B-per I-per I-per I-pe...  \n",
       "16  O O O O B-tim O O O O O O O O O O O O O O O B-...  \n",
       "17  O O O O O O O B-org I-org I-org I-org I-org O ...  \n",
       "18        O O O B-org O O O O O O O O O O O O O O O O  \n",
       "19  O O O O O O O O O O O O B-gpe O B-per I-per O ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sen = data[200:220]\n",
    "test_sen = test_sen.reset_index(drop=True)\n",
    "test_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class process_sentence_single(Dataset):\n",
    "\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        print(\"dataloader initialized\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        sentence = self.text.strip().split() \n",
    "        \n",
    "        tokenized_sentence = []\n",
    "\n",
    "        for word in sentence:\n",
    "            # Tokenize the word and count # of subwords the word is broken into\n",
    "            tokenized_word = tokenizer.tokenize(word)\n",
    "            tokenized_sentence.extend(tokenized_word)\n",
    "        \n",
    "        \n",
    "        sen_code = tokenizer.encode_plus(tokenized_sentence,    \n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "#             max_length = 128,  # maximum length of a sentence\n",
    "#             pad_to_max_length=True,  # Add [PAD]s\n",
    "            return_attention_mask = True,  # Generate the attention mask\n",
    "#             return_tensors = 'pt'\n",
    "            )\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in sen_code.items()}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class process_sentence_batch(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "#         print(\"dataloader initialized\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        sentence = self.data['text'][idx].strip().split() \n",
    "        \n",
    "        tokenized_sentence = []\n",
    "\n",
    "        for word in sentence:\n",
    "            # Tokenize the word and count # of subwords the word is broken into\n",
    "            tokenized_word = tokenizer.tokenize(word)\n",
    "            tokenized_sentence.extend(tokenized_word)\n",
    "        \n",
    "        \n",
    "        sen_code = tokenizer.encode_plus(tokenized_sentence,    \n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "            max_length = 128,  # maximum length of a sentence\n",
    "            pad_to_max_length=True,  # Add [PAD]s\n",
    "            return_attention_mask = True,  # Generate the attention mask\n",
    "#             return_tensors = 'pt'\n",
    "            )\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in sen_code.items()}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_text(test_datas_batch, model):\n",
    "    for i,sample in enumerate(test_datas_batch):\n",
    "        ids=sample['input_ids'].to(device)\n",
    "        mask=sample['attention_mask'].to(device)\n",
    "        pred = model2(input_ids=ids, attention_mask=mask)\n",
    "\n",
    "        return ids, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_things=[]\n",
    "def make_batch_pred(test_sen):\n",
    "    pre_text = process_sentence_batch(test_sen)\n",
    "    test_datas_batch = DataLoader(pre_text, batch_size = 8, shuffle=False)\n",
    "\n",
    "    ids, preds = infer_text(test_datas_batch, model2)\n",
    "\n",
    "\n",
    "    flattened_predictions = []\n",
    "    for logit in preds['logits']:\n",
    "        active_logits = logit.view(-1, model2.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions.append(torch.argmax(active_logits, axis=1)) # shape (batch_size * seq_len,)\n",
    "    # flattened_predictions\n",
    "\n",
    "    for i, predict in  enumerate(flattened_predictions):\n",
    "        text_tokens= tokenizer.convert_ids_to_tokens(ids[i])\n",
    "        sep_i = text_tokens.index('[SEP]')\n",
    "        text_labels = []\n",
    "        for i in predict.squeeze(0).cpu().numpy():\n",
    "            text_labels.append(ids_to_label.get(i))\n",
    "\n",
    "\n",
    "        text_tokens = text_tokens[1:sep_i]\n",
    "        text_labels = text_labels[1:sep_i]\n",
    "\n",
    "\n",
    "        print(\"\\n printing tokens with labels\")\n",
    "        print(text_tokens)\n",
    "        print(text_labels)\n",
    "        \n",
    "        print(len(text_tokens))\n",
    "        print(len(text_labels))\n",
    "\n",
    "        sent = []\n",
    "        \n",
    "        for text in text_tokens:\n",
    "            if text.startswith('##'):\n",
    "                sent[-1] = sent[-1]+text[2:]\n",
    "            \n",
    "        \n",
    "        per=[]\n",
    "        geo=[]\n",
    "        org=[]\n",
    "\n",
    "        for text, label in zip(text_tokens,text_labels):\n",
    "            print(text,label)\n",
    "\n",
    "            if(label[2:] == 'per'):\n",
    "                if text.startswith('##'):\n",
    "                    per[-1] = per[-1]+text[2:]\n",
    "                else:\n",
    "                    per.append(text)\n",
    "\n",
    "            if(label[2:] == 'geo'):\n",
    "                if text.startswith('##'):\n",
    "                    geo[-1] = geo[-1]+text[2:]\n",
    "                else:\n",
    "                    geo.append(text)\n",
    "\n",
    "            if(label[2:] == 'org'):\n",
    "                if text.startswith('##'):\n",
    "                    org[-1] = org[-1]+text[2:]\n",
    "                else:\n",
    "                    org.append(text)\n",
    "        \n",
    "        all_things.append({'sent':sent,\n",
    "            'per':per, \n",
    "            'geo':geo,\n",
    "            'org':org})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_single_pred(sentence):\n",
    "\n",
    "    # get the processed input_ids and mask\n",
    "    # test_text = \"Mark is the ceo of Facebook. located in California .\"\n",
    "    test_text = sentence\n",
    "    pre_text = process_sentence_single(test_text)\n",
    "    text= pre_text[0]\n",
    "\n",
    "    ids = text ['input_ids']\n",
    "    mask = text ['attention_mask']\n",
    "\n",
    "    \n",
    "    #make prediction\n",
    "    \n",
    "    test_pred = model2(input_ids=torch.unsqueeze(ids,0).to(device), attention_mask=torch.unsqueeze(mask,0).to(device))\n",
    "\n",
    "    \n",
    "    ## flatten prediction\n",
    "    active_logits = test_pred[0].view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "    print(\"\\nFlatten Predictions.....\\n\")\n",
    "    print(flattened_predictions)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"\\n printing tokens.....\")\n",
    "    for i in torch.unsqueeze(ids,0):\n",
    "        print(tokenizer.convert_ids_to_tokens(i))\n",
    "\n",
    "    # convert ids to corresponding tokens\n",
    "    text_tokens= tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "    # convert predctions to labels\n",
    "    text_labels = []\n",
    "    for i in flattened_predictions.squeeze(0).cpu().numpy():\n",
    "        text_labels.append(ids_to_label.get(i))\n",
    "\n",
    "#     print(\"\\n printing predicted token labels.....\")\n",
    "#     print(text_labels)\n",
    "\n",
    "    # remove first and last tokens ([CLS] and [SEP])\n",
    "    text_tokens = text_tokens[1:-1]\n",
    "    text_labels = text_labels[1:-1]\n",
    "\n",
    "\n",
    "    print(\"\\n printing tokens with labels\")\n",
    "    print(text_tokens)\n",
    "    print(text_labels)\n",
    "    \n",
    "    return text_tokens, text_labels\n",
    "#     print(\"\\n printing zipped tokens with labels.....\\n\")\n",
    "#     for token, label in zip(text_tokens,text_labels):\n",
    "#         print(token,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt, lbl = make_single_pred(\"Sundar Pichai lived in India is CEO of Google .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Free flow of Text','Extracted Name','Extracted Location','Extracted Organization'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in test_sen['text']:\n",
    "#     print(sent)\n",
    "    text_sen = sent\n",
    "    per=[]\n",
    "    geo=[]\n",
    "    org=[]\n",
    "    txt, lbl = make_single_pred(sent)\n",
    "    for text, label in zip(txt,lbl):\n",
    "#         print(text,label)\n",
    "        \n",
    "        if(label == 'I-per'):\n",
    "            if not text.startswith('##'):\n",
    "#                 print(\"####\")\n",
    "#                 print(text)\n",
    "                if(len(per)<=0):\n",
    "                    per.append(text)\n",
    "                else:\n",
    "                    per[-1] = per[-1]+' '+ text\n",
    "                continue\n",
    "\n",
    "        if(label[2:] == 'per'):\n",
    "            if text.startswith('##'):\n",
    "                per[-1] = per[-1]+text[2:]\n",
    "            else:\n",
    "                per.append(text)\n",
    "\n",
    "                \n",
    "                \n",
    "        if(label == 'I-geo'):\n",
    "            \n",
    "            if not text.startswith('##'):\n",
    "#                 print(\"####\")\n",
    "#                 print(text)\n",
    "                if(len(geo)<=0):\n",
    "                    geo.append(text)\n",
    "                else:\n",
    "                    geo[-1] = geo[-1]+' '+ text\n",
    "                \n",
    "                continue\n",
    "                \n",
    "        if(label[2:] == 'geo'):\n",
    "            if text.startswith('##'):\n",
    "                geo[-1] = geo[-1]+text[2:]\n",
    "            else:\n",
    "                geo.append(text)\n",
    "\n",
    "                \n",
    "                \n",
    "        if(label == 'I-org'):\n",
    "            if not text.startswith('##'):\n",
    "#                 print(\"####\")\n",
    "#                 print(text)\n",
    "                if(len(org)<=0):\n",
    "                    org.append(text)\n",
    "                else:\n",
    "                    org[-1] = org[-1]+' '+ text\n",
    "                continue\n",
    "                \n",
    "        if(label[2:] == 'org'):\n",
    "            if text.startswith('##'):\n",
    "                org[-1] = org[-1]+text[2:]\n",
    "            else:\n",
    "                org.append(text)\n",
    "                \n",
    "#     df.append({'Free flow of Text':text_sen, 'Extracted Name':per, 'Extracted Location':geo,'Extracted Organization':org}, ignore_index=True)\n",
    "        \n",
    "    new_record = pd.DataFrame([[text_sen,per,geo,org]], columns = ['Free flow of Text','Extracted Name','Extracted Location','Extracted Organization'])\n",
    "\n",
    "    df = pd.concat([df, new_record])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = Ner_Data(test_dataset)\n",
    "test_data_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "            preds= model2(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "\n",
    "            loss = preds['loss']\n",
    "            eval_logits = preds['logits'] \n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            \n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        \n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(labels)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    labels = [ids_to_label[id.item()] for id in eval_labels]\n",
    "    predictions = [ids_to_label[id.item()] for id in eval_preds]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, predictions = valid(model2, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report([labels], [predictions]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f631b86adf457f338a4428257848576e298574cd5d4f6dd6f5d4f2a8f96b5adb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
